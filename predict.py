{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da5c354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = re.sub(r'[\\u064B-\\u065F]', '', text)  # Remove tashkeel\n",
    "    text = text.replace(\"Ø£\", \"Ø§\").replace(\"Ø¥\", \"Ø§\").replace(\"Ø¢\", \"Ø§\")\n",
    "    text = text.replace(\"Ø©\", \"Ù‡\")\n",
    "    text = text.replace(\"ÙŠ\", \"Ù‰\")\n",
    "    return text\n",
    "\n",
    "def clean_corpus(corpus, words_to_remove):\n",
    "    cleaned_corpus = []\n",
    "    for text in corpus:\n",
    "        if isinstance(text, str):\n",
    "            text = normalize_arabic(text)\n",
    "            for word in words_to_remove:\n",
    "                word = normalize_arabic(word)\n",
    "                pattern = r'\\b' + re.sub(r'(.)\\1*', r'\\1+', re.escape(word)) + r'\\b'\n",
    "                text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.UNICODE)\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        else:\n",
    "            text = \"\"\n",
    "        cleaned_corpus.append(text)\n",
    "    return cleaned_corpus\n",
    "\n",
    "# Load model and vectorizer\n",
    "extract_path = \"product_matching_model\"\n",
    "model_path = os.path.join(extract_path, \"product_matching_model.pkl\")\n",
    "vectorizer_path = os.path.join(extract_path, \"vectorizer.pkl\")\n",
    "\n",
    "print(\"ğŸ”¹ Loading Model & Vectorizer...\")\n",
    "model = joblib.load(model_path)\n",
    "vectorizer = joblib.load(vectorizer_path)\n",
    "\n",
    "def product_matching_pipeline(excel_file_path, masterfile_sheet, dataset_sheet, words_to_remove):\n",
    "    print(\"ğŸ”¹ Loading Excel File...\")\n",
    "    try:\n",
    "        masterfile = pd.read_excel(excel_file_path, sheet_name=masterfile_sheet)\n",
    "        dataset = pd.read_excel(excel_file_path, sheet_name=dataset_sheet)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading Excel file: {e}\")\n",
    "        return None\n",
    "\n",
    "    required_master_cols = {'product_name_ar', 'sku'}\n",
    "    required_dataset_cols = {'seller_item_name', 'marketplace_product_name_ar'}\n",
    "\n",
    "    if not required_master_cols.issubset(masterfile.columns):\n",
    "        print(f\"âŒ Missing columns in Master File: {required_master_cols - set(masterfile.columns)}\")\n",
    "        return None\n",
    "    if not required_dataset_cols.issubset(dataset.columns):\n",
    "        print(f\"âŒ Missing columns in Dataset: {required_dataset_cols - set(dataset.columns)}\")\n",
    "        return None\n",
    "\n",
    "    print(\"ğŸ”¹ Cleaning Text Data...\")\n",
    "    masterfile['marketplace_name_clean'] = clean_corpus(masterfile['product_name_ar'].astype(str), words_to_remove)\n",
    "    dataset['seller_item_name_clean'] = clean_corpus(dataset['seller_item_name'].astype(str), words_to_remove)\n",
    "    dataset['marketplace_name_clean'] = clean_corpus(dataset['marketplace_product_name_ar'].astype(str), words_to_remove)\n",
    "\n",
    "    print(\"ğŸ”¹ Transforming Data...\")\n",
    "    X_dataset = vectorizer.transform(dataset['seller_item_name_clean'])\n",
    "\n",
    "    print(\"ğŸ”¹ Predicting Matches...\")\n",
    "    y_pred_dataset = model.predict(X_dataset)\n",
    "    y_pred_proba = model.predict_proba(X_dataset)\n",
    "    confidence_scores = np.max(y_pred_proba, axis=1)\n",
    "\n",
    "    dataset['predicted_marketplace_name'] = y_pred_dataset\n",
    "    dataset['confidence_score'] = confidence_scores\n",
    "\n",
    "    sku_map = masterfile.set_index('marketplace_name_clean')['sku'].to_dict()\n",
    "    dataset['matched_sku'] = dataset['predicted_marketplace_name'].apply(lambda x: sku_map.get(x, 'Not Found'))\n",
    "\n",
    "    dataset = dataset.drop(columns=['seller_item_name_clean', 'marketplace_name_clean', 'predicted_marketplace_name'])\n",
    "    return dataset\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    words_to_remove = ['Ø´Ø±ÙŠØ·', 'Ø¬Ø¯ÙŠØ¯', 'Ù‚Ø¯ÙŠÙ…', 'Ø³Ø¹Ø±', 'Ø³Ø§Ù†ÙˆÙÙŠ', 'Ø§ÙÙ†ØªØ³', 'Ø§Ø¨ÙŠÙƒÙˆ', 'Ø¬', 'Ø³', \n",
    "        'Ø§Ù„Ø¹Ø§Ù…Ø±ÙŠØ©', 'ÙƒØ¨ÙŠØ±', 'ØµØºÙŠØ±', 'Ù‡Ø§Ù…', 'Ù…Ù‡Ù…', 'Ø§Ø­Ø°Ø±', 'ÙŠÙˆØªÙˆØ¨ÙŠØ§', 'Ø¯ÙˆØ§', \n",
    "        'Ø§Ø¯ÙˆÙŠØ§', 'Ù„Ø§ ÙŠØ±ØªØ¬Ø¹', 'ÙŠØ±ØªØ¬Ø¹', 'Ø¹Ø§Ø¯ÙŠ', 'Ù…ÙŠØ¨Ø§ÙƒÙˆ']\n",
    "\n",
    "    input_file = \"input.xlsx\"\n",
    "    output_file = \"final_matched_dataset.xlsx\"\n",
    "\n",
    "    print(\"ğŸš€ Starting Product Matching Process...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    final_dataset = product_matching_pipeline(\n",
    "        excel_file_path=input_file,\n",
    "        masterfile_sheet=\"Master File\",\n",
    "        dataset_sheet=\"Dataset\",\n",
    "        words_to_remove=words_to_remove\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    if final_dataset is not None:\n",
    "        final_dataset.to_excel(output_file, index=False)\n",
    "        print(f\"âœ… Processing completed in {end_time - start_time:.2f} seconds.\")\n",
    "        print(f\"ğŸ“‚ Results saved in {output_file}\")\n",
    "    else:\n",
    "        print(\"âŒ Error: Could not process the dataset.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
