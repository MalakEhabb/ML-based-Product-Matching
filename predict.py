{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import re\n",
    "import time\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = re.sub(r'[\\u064B-\\u065F]', '', text)  # Remove tashkeel\n",
    "    text = text.replace(\"أ\", \"ا\").replace(\"إ\", \"ا\").replace(\"آ\", \"ا\")\n",
    "    text = text.replace(\"ة\", \"ه\")\n",
    "    text = text.replace(\"ي\", \"ى\")\n",
    "    return text\n",
    "\n",
    "def clean_corpus(corpus, words_to_remove):\n",
    "    cleaned_corpus = []\n",
    "    for text in corpus:\n",
    "        if isinstance(text, str):\n",
    "            text = normalize_arabic(text)\n",
    "            for word in words_to_remove:\n",
    "                word = normalize_arabic(word)\n",
    "                pattern = r'\\b' + re.sub(r'(.)\\1*', r'\\1+', re.escape(word)) + r'\\b'\n",
    "                text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.UNICODE)\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        else:\n",
    "            text = \"\"\n",
    "        cleaned_corpus.append(text)\n",
    "    return cleaned_corpus\n",
    "\n",
    "# Unzipping step\n",
    "zip_path = \"product_matching_model.zip\"\n",
    "extract_path = \"product_matching_model\"\n",
    "\n",
    "if not os.path.exists(extract_path):\n",
    "    print(\"📦 Extracting model files...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(\"✅ Extraction completed.\")\n",
    "\n",
    "# Load model and vectorizer\n",
    "model_path = os.path.join(extract_path, \"product_matching_model.pkl\")\n",
    "vectorizer_path = os.path.join(extract_path, \"vectorizer.pkl\")\n",
    "\n",
    "print(\"🔹 Loading Model & Vectorizer...\")\n",
    "model = joblib.load(model_path)\n",
    "vectorizer = joblib.load(vectorizer_path)\n",
    "\n",
    "def product_matching_pipeline(excel_file_path, masterfile_sheet, dataset_sheet, words_to_remove):\n",
    "    print(\"🔹 Loading Excel File...\")\n",
    "    try:\n",
    "        masterfile = pd.read_excel(excel_file_path, sheet_name=masterfile_sheet)\n",
    "        dataset = pd.read_excel(excel_file_path, sheet_name=dataset_sheet)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading Excel file: {e}\")\n",
    "        return None\n",
    "\n",
    "    required_master_cols = {'product_name_ar', 'sku'}\n",
    "    required_dataset_cols = {'seller_item_name', 'marketplace_product_name_ar'}\n",
    "\n",
    "    if not required_master_cols.issubset(masterfile.columns):\n",
    "        print(f\"❌ Missing columns in Master File: {required_master_cols - set(masterfile.columns)}\")\n",
    "        return None\n",
    "    if not required_dataset_cols.issubset(dataset.columns):\n",
    "        print(f\"❌ Missing columns in Dataset: {required_dataset_cols - set(dataset.columns)}\")\n",
    "        return None\n",
    "\n",
    "    print(\"🔹 Cleaning Text Data...\")\n",
    "    masterfile['marketplace_name_clean'] = clean_corpus(masterfile['product_name_ar'].astype(str), words_to_remove)\n",
    "    dataset['seller_item_name_clean'] = clean_corpus(dataset['seller_item_name'].astype(str), words_to_remove)\n",
    "    dataset['marketplace_name_clean'] = clean_corpus(dataset['marketplace_product_name_ar'].astype(str), words_to_remove)\n",
    "\n",
    "    print(\"🔹 Transforming Data...\")\n",
    "    X_dataset = vectorizer.transform(dataset['seller_item_name_clean'])\n",
    "\n",
    "    print(\"🔹 Predicting Matches...\")\n",
    "    y_pred_dataset = model.predict(X_dataset)\n",
    "    y_pred_proba = model.predict_proba(X_dataset)\n",
    "    confidence_scores = np.max(y_pred_proba, axis=1)\n",
    "\n",
    "    dataset['predicted_marketplace_name'] = y_pred_dataset\n",
    "    dataset['confidence_score'] = confidence_scores\n",
    "\n",
    "    sku_map = masterfile.set_index('marketplace_name_clean')['sku'].to_dict()\n",
    "    dataset['matched_sku'] = dataset['predicted_marketplace_name'].apply(lambda x: sku_map.get(x, 'Not Found'))\n",
    "\n",
    "    dataset = dataset.drop(columns=['seller_item_name_clean', 'marketplace_name_clean', 'predicted_marketplace_name'])\n",
    "    return dataset\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    words_to_remove = ['شريط', 'جديد', 'قديم', 'سعر', 'سانوفي', 'افنتس', 'ابيكو', 'ج', 'س', \n",
    "        'العامرية', 'كبير', 'صغير', 'هام', 'مهم', 'احذر', 'يوتوبيا', 'دوا', \n",
    "        'ادويا', 'لا يرتجع', 'يرتجع', 'عادي', 'ميباكو']\n",
    "\n",
    "    input_file = \"input.xlsx\"\n",
    "    output_file = \"final_matched_dataset.xlsx\"\n",
    "\n",
    "    print(\"🚀 Starting Product Matching Process...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    final_dataset = product_matching_pipeline(\n",
    "        excel_file_path=input_file,\n",
    "        masterfile_sheet=\"Master File\",\n",
    "        dataset_sheet=\"Dataset\",\n",
    "        words_to_remove=words_to_remove\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    if final_dataset is not None:\n",
    "        final_dataset.to_excel(output_file, index=False)\n",
    "        print(f\"✅ Processing completed in {end_time - start_time:.2f} seconds.\")\n",
    "        print(f\"📂 Results saved in {output_file}\")\n",
    "    else:\n",
    "        print(\"❌ Error: Could not process the dataset.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
