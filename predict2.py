import pandas as pd
import numpy as np
import joblib
import re
import time
import sys
import os

def normalize_arabic(text):
    if not isinstance(text, str):
        return text
    text = re.sub(r'[\u064B-\u065F]', '', text)  # Remove tashkeel
    text = text.replace("Ø£", "Ø§").replace("Ø¥", "Ø§").replace("Ø¢", "Ø§")
    text = text.replace("Ø©", "Ù‡")
    text = text.replace("ÙŠ", "Ù‰")
    return text

def clean_corpus(corpus, words_to_remove):
    cleaned_corpus = []
    for text in corpus:
        if isinstance(text, str):
            text = normalize_arabic(text)
            for word in words_to_remove:
                word = normalize_arabic(word)
                pattern = r'\b' + re.sub(r'(.)\1*', r'\1+', re.escape(word)) + r'\b'
                text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.UNICODE)
            text = re.sub(r'\s+', ' ', text).strip()
        else:
            text = ""
        cleaned_corpus.append(text)
    return cleaned_corpus

# Load model and vectorizer
extract_path = "product_matching_model"
model_path = r"product_matching_model/product_matching_model.pkl"
vectorizer_path = r"vectorizer.pkl"

print("ğŸ”¹ Loading Model & Vectorizer...")
model = joblib.load(model_path)
vectorizer = joblib.load(vectorizer_path)

def product_matching_pipeline(excel_file_path, masterfile_sheet, dataset_sheet, words_to_remove):
    print("ğŸ”¹ Loading Excel File...")
    try:
        masterfile = pd.read_excel(excel_file_path, sheet_name=masterfile_sheet)
        dataset = pd.read_excel(excel_file_path, sheet_name=dataset_sheet)
    except Exception as e:
        print(f"âŒ Error loading Excel file: {e}")
        return None

    required_master_cols = {'product_name_ar', 'sku'}
    required_dataset_cols = {'seller_item_name', 'marketplace_product_name_ar'}

    if not required_master_cols.issubset(masterfile.columns):
        print(f"âŒ Missing columns in Master File: {required_master_cols - set(masterfile.columns)}")
        return None
    if not required_dataset_cols.issubset(dataset.columns):
        print(f"âŒ Missing columns in Dataset: {required_dataset_cols - set(dataset.columns)}")
        return None

    print("ğŸ”¹ Cleaning Text Data...")
    masterfile['marketplace_name_clean'] = clean_corpus(masterfile['product_name_ar'].astype(str), words_to_remove)
    dataset['seller_item_name_clean'] = clean_corpus(dataset['seller_item_name'].astype(str), words_to_remove)
    dataset['marketplace_name_clean'] = clean_corpus(dataset['marketplace_product_name_ar'].astype(str), words_to_remove)

    print("ğŸ”¹ Transforming Data...")
    X_dataset = vectorizer.transform(dataset['seller_item_name_clean'])

    print("ğŸ”¹ Predicting Matches...")
    y_pred_dataset = model.predict(X_dataset)
    y_pred_proba = model.predict_proba(X_dataset)
    confidence_scores = np.max(y_pred_proba, axis=1)

    dataset['predicted_marketplace_name'] = y_pred_dataset
    dataset['confidence_score'] = confidence_scores

    sku_map = masterfile.set_index('marketplace_name_clean')['sku'].to_dict()
    dataset['matched_sku'] = dataset['predicted_marketplace_name'].apply(lambda x: sku_map.get(x, 'Not Found'))

    dataset = dataset.drop(columns=['seller_item_name_clean', 'marketplace_name_clean', 'predicted_marketplace_name'])
    return dataset

if __name__ == "__main__":
    if len(sys.argv) != 4:
        print("Usage: python predict.py <file.xlsx> <MasterSheet> <DatasetSheet>")
        sys.exit(1)
    
    input_file = sys.argv[1]
    master_sheet = sys.argv[2]
    dataset_sheet = sys.argv[3]
    output_file = "final_matched_dataset.xlsx"
    
    words_to_remove = ['Ø´Ø±ÙŠØ·', 'Ø¬Ø¯ÙŠØ¯', 'Ù‚Ø¯ÙŠÙ…', 'Ø³Ø¹Ø±', 'Ø³Ø§Ù†ÙˆÙÙŠ', 'Ø§ÙÙ†ØªØ³', 'Ø§Ø¨ÙŠÙƒÙˆ', 'Ø¬', 'Ø³', 
        'Ø§Ù„Ø¹Ø§Ù…Ø±ÙŠØ©', 'ÙƒØ¨ÙŠØ±', 'ØµØºÙŠØ±', 'Ù‡Ø§Ù…', 'Ù…Ù‡Ù…', 'Ø§Ø­Ø°Ø±', 'ÙŠÙˆØªÙˆØ¨ÙŠØ§', 'Ø¯ÙˆØ§', 
        'Ø§Ø¯ÙˆÙŠØ§', 'Ù„Ø§ ÙŠØ±ØªØ¬Ø¹', 'ÙŠØ±ØªØ¬Ø¹', 'Ø¹Ø§Ø¯ÙŠ', 'Ù…ÙŠØ¨Ø§ÙƒÙˆ']
    
    print("ğŸš€ Starting Product Matching Process...")
    start_time = time.time()
    
    final_dataset = product_matching_pipeline(input_file, master_sheet, dataset_sheet, words_to_remove)
    
    end_time = time.time()
    
    if final_dataset is not None:
        final_dataset.to_excel(output_file, index=False)
        print(f"âœ… Processing completed in {end_time - start_time:.2f} seconds.")
        print(f"ğŸ“‚ Results saved in {output_file}")
    else:
        print("âŒ Error: Could not process the dataset.")
